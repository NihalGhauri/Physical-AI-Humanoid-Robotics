# #!/usr/bin/env python3
# """
# RAG Agent for Book Queries

# This module implements a RAG (Retrieval-Augmented Generation) agent that uses OpenAI's Agent SDK
# to answer book-related questions. The agent integrates with Qdrant for vector search and Cohere
# for embeddings, ensuring responses are grounded only in retrieved content with proper source attribution.
# """

# import os
# import logging
# import time
# from typing import List, Dict, Any, Optional
# from dataclasses import dataclass
# from dotenv import load_dotenv
# from collections import defaultdict

# # Load environment variables
# load_dotenv()

# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)


# @dataclass
# class Query:
#     """Represents a user's natural language question to the RAG agent."""
#     text: str
#     timestamp: str = None


# @dataclass
# class RetrievedChunk:
#     """Represents a content chunk retrieved from the knowledge base."""
#     chunk_id: str
#     content: str
#     source_url: str
#     page_title: str
#     section_heading: str
#     chunk_index: int
#     score: float
#     original_chunk_id: str


# @dataclass
# class AgentResponse:
#     """Represents the response generated by the RAG agent."""
#     content: str
#     sources: List[str]
#     grounded: bool
#     retrieved_chunks: List[RetrievedChunk]


# class AgentConfig:
#     """Configuration class for the RAG Agent."""

#     def __init__(self):
#         # Load configuration from environment
#         self.openai_api_key = os.getenv("OPENAI_API_KEY")
#         self.gemini_api_key = os.getenv("GEMINI_API_KEY")  # Added Gemini API key support
#         self.cohere_api_key = os.getenv("COHERE_API_KEY")
#         self.qdrant_url = os.getenv("QDRANT_URL")
#         self.qdrant_api_key = os.getenv("QDRANT_API_KEY")
#         self.collection_name = os.getenv("QDRANT_COLLECTION_NAME", "Rag_Chatbot_book")
#         self.temperature = float(os.getenv("AGENT_TEMPERATURE", "0.7"))
#         self.max_tokens = int(os.getenv("AGENT_MAX_TOKENS", "2048"))
#         self.top_k = int(os.getenv("AGENT_TOP_K", "5"))
#         self.min_score = float(os.getenv("AGENT_MIN_SCORE", "0.3"))

#         # Model selection - use Gemini if available, otherwise OpenAI
#         self.use_gemini = bool(self.gemini_api_key)
#         self.model_name = os.getenv("AGENT_MODEL_NAME", "gpt-3.5-turbo" if not self.use_gemini else "gemini-2.0-flash")

#         # Validate required configuration
#         if not self.openai_api_key and not self.gemini_api_key:
#             raise ValueError("Either OPENAI_API_KEY or GEMINI_API_KEY environment variable is required")
#         if not self.cohere_api_key:
#             raise ValueError("COHERE_API_KEY environment variable is required")
#         if not self.qdrant_url:
#             raise ValueError("QDRANT_URL environment variable is required")
#         if not self.qdrant_api_key:
#             raise ValueError("QDRANT_API_KEY environment variable is required")


# from functools import lru_cache
# import hashlib

# class RetrievalWrapper:
#     """Wrapper for the RetrievalValidator to provide a clean interface for retrieval operations."""

#     def __init__(self, retrieval_validator):
#         self.retrieval_validator = retrieval_validator
#         self.logger = logging.getLogger(__name__)
#         # Initialize cache for repeated queries
#         self.query_cache = {}

#     def retrieve_chunks(self, query: str, top_k: int = 5) -> List[RetrievedChunk]:
#         """Retrieve relevant chunks for a given query."""
#         try:
#             return self.retrieval_validator.retrieve_chunks(query, top_k)
#         except Exception as e:
#             self.logger.error(f"Error retrieving chunks: {e}")
#             return []

#     def _get_cache_key(self, query: str, top_k: int) -> str:
#         """Generate a cache key for the given query and top_k."""
#         cache_input = f"{query}_{top_k}"
#         return hashlib.md5(cache_input.encode()).hexdigest()
#     def retrieve_chunks_with_caching(self, query: str, top_k: int = 5) -> List[RetrievedChunk]:
#         """Retrieve relevant chunks for a given query with caching."""
#         cache_key = self._get_cache_key(query, top_k)

#         # Check if result is in cache
#         if cache_key in self.query_cache:
#             self.logger.info(f"Cache hit for query: {query[:50]}...")
#             return self.query_cache[cache_key]

#         # Not in cache, retrieve from source
#         try:
#             chunks = self.retrieval_validator.retrieve_chunks(query, top_k)
#             # Cache the result
#             self.query_cache[cache_key] = chunks
#             self.logger.info(f"Cached result for query: {query[:50]}...")
#             return chunks
#         except Exception as e:
#             self.logger.error(f"Error retrieving chunks: {e}")
#             return []

#     def retrieve_chunks_with_error_handling(self, query: str, top_k: int = 5, max_retries: int = 3) -> List[RetrievedChunk]:
#         """Retrieve relevant chunks for a given query with error handling and retries."""
#         for attempt in range(max_retries):
#             try:
#                 chunks = self.retrieve_chunks_with_caching(query, top_k)
#                 self.logger.info(f"Successfully retrieved {len(chunks)} chunks on attempt {attempt + 1}")
#                 return chunks
#             except Exception as e:
#                 self.logger.error(f"Attempt {attempt + 1} failed to retrieve chunks: {e}")
#                 if attempt == max_retries - 1:  # Last attempt
#                     self.logger.error("All retrieval attempts failed")
#                     return []
#                 # Could add delay between retries here
#         return []

#     def validate_retrieval(self) -> bool:
#         """Validate that the retrieval system is functioning correctly."""
#         try:
#             return self.retrieval_validator.validate_connection()
#         except Exception as e:
#             self.logger.error(f"Error validating retrieval system: {e}")
#             return False


# class RAGAgent:
#     """RAG Agent that answers book-related questions based on retrieved content."""

#     def __init__(self, config: AgentConfig = None):
#         """Initialize the RAG Agent with required configurations."""
#         logger.info("Initializing RAG Agent")

#         # Use provided config or create default
#         self.config = config or AgentConfig()

#         # Initialize OpenAI client
#         import openai
#         if self.config.use_gemini:
#             # Use Gemini API with OpenAI-compatible endpoint
#             self.openai_client = openai.AsyncOpenAI(
#                 api_key=self.config.gemini_api_key,
#                 base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
#             )
#             self.is_async_client = True
#         else:
#             self.openai_client = openai.OpenAI(api_key=self.config.openai_api_key)
#             self.is_async_client = False

#         # Import OpenAI Agents SDK
#         try:
#             from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, RunConfig
#             self.agents_available = True
#             self.Agent = Agent
#             self.Runner = Runner
#             self.AsyncOpenAI = AsyncOpenAI
#             self.OpenAIChatCompletionsModel = OpenAIChatCompletionsModel
#             self.RunConfig = RunConfig
#         except ImportError:
#             logger.warning("OpenAI Agents SDK not available. Using fallback to Chat Completions.")
#             self.agents_available = False

#         # Initialize Cohere client
#         import cohere
#         self.cohere_client = cohere.Client(self.config.cohere_api_key)

#         # Define system instructions for grounded responses
#         self.system_instructions = """
#         You are a helpful book assistant that answers questions based only on the retrieved content provided to you.
#         You MUST follow these rules:
#         1. Only use information from the retrieved content to answer questions
#         2. If the retrieved content does not contain information to answer the question, explicitly state that you cannot answer due to lack of relevant context
#         3. Always cite the source URLs of the information you use in your response
#         4. Do not generate answers from your general knowledge or training data
#         5. If you're unsure whether information comes from the retrieved content or your general knowledge, do not use it
#         6. Be concise but thorough in your responses based on the provided context
#         """

#         # Initialize rate limiting
#         self.request_timestamps = defaultdict(list)
#         self.rate_limit_window = 60  # 1 minute window
#         self.max_requests_per_minute = 100  # Configurable rate limit

#         logger.info("RAG Agent initialized successfully")

#         # Initialize retrieval validator
#         self.initialize_retrieval_validator()

#         # Create a wrapper for the retrieval validator
#         self.retrieval_wrapper = RetrievalWrapper(self.retrieval_validator)

#     def _check_rate_limit(self, client_id: str = "default") -> bool:
#         """
#         Check if the client has exceeded the rate limit.

#         Args:
#             client_id: Identifier for the client making the request

#         Returns:
#             True if within rate limit, False if exceeded
#         """
#         current_time = time.time()

#         # Clean up old timestamps outside the window
#         self.request_timestamps[client_id] = [
#             timestamp for timestamp in self.request_timestamps[client_id]
#             if current_time - timestamp < self.rate_limit_window
#         ]

#         # Check if we're within the rate limit
#         if len(self.request_timestamps[client_id]) >= self.max_requests_per_minute:
#             logger.warning(f"Rate limit exceeded for client: {client_id}")
#             return False

#         # Add current request timestamp
#         self.request_timestamps[client_id].append(current_time)
#         return True

#     def _validate_request(self, question: str) -> tuple[bool, str]:
#         """
#         Validate the incoming request.

#         Args:
#             question: The question to validate

#         Returns:
#             Tuple of (is_valid, error_message)
#         """
#         if not question or not question.strip():
#             return False, "Question cannot be empty"

#         if len(question.strip()) < 3:
#             return False, "Question is too short (minimum 3 characters)"

#         if len(question.strip()) > 1000:
#             return False, "Question is too long (maximum 1000 characters)"

#         # Additional validation could be added here
#         return True, ""

#     def initialize_retrieval_validator(self):
#         """Initialize the retrieval validator separately to handle import issues."""
#         try:
#             from retrieve import RetrievalValidator
#             self.retrieval_validator = RetrievalValidator()
#             logger.info("Retrieval validator initialized successfully")
#         except ImportError as e:
#             logger.error(f"Failed to import retrieval validator: {e}")
#             raise

#     def validate_query(self, query: str) -> bool:
#         """Validate that the query is appropriate for the RAG agent."""
#         if not query or not query.strip():
#             logger.warning("Query is empty or None")
#             return False

#         if len(query.strip()) < 3:
#             logger.warning("Query is too short (less than 3 characters)")
#             return False

#         if len(query.strip()) > 1000:
#             logger.warning("Query is too long (more than 1000 characters)")
#             return False

#         return True

#     def preprocess_query(self, query: str) -> str:
#         """Preprocess the query before processing."""
#         # Clean and normalize the query
#         processed_query = query.strip()
#         # Remove extra whitespace
#         processed_query = ' '.join(processed_query.split())
#         return processed_query

#     def _generate_response(self, question: str, retrieved_chunks: List[RetrievedChunk]) -> str:
#         """
#         Generate a response based on the question and retrieved chunks using OpenAI API.
#         If OpenAI Agents SDK is available, use it; otherwise, fall back to Chat Completions.

#         Args:
#             question: The original question asked
#             retrieved_chunks: List of relevant chunks retrieved from the knowledge base

#         Returns:
#             Generated response string
#         """
#         if self.agents_available:
#             return self._generate_response_with_agents(question, retrieved_chunks)
#         else:
#             return self._generate_response_with_chat_completions(question, retrieved_chunks)

#     def _generate_response_with_agents(self, question: str, retrieved_chunks: List[RetrievedChunk]) -> str:
#         """
#         Generate a response using the OpenAI Agents SDK.

#         Args:
#             question: The original question asked
#             retrieved_chunks: List of relevant chunks retrieved from the knowledge base

#         Returns:
#             Generated response string
#         """
#         try:
#             # Format the context from retrieved chunks
#             context = self._format_context_for_llm(retrieved_chunks)

#             # Prepare the input for the agent
#             input_text = f"Question: {question}\n\nContext: {context}\n\nPlease provide an answer based only on the provided context, citing the sources used."

#             # Import required modules
#             from agents import function_tool
#             import asyncio
#             from pydantic import BaseModel

#             # Define a function tool for retrieving information from the knowledge base
#             @function_tool
#             def retrieve_information(query: str) -> str:
#                 """Retrieve relevant information from the knowledge base based on the query.

#                 Args:
#                     query: The search query to find relevant information
#                 """
#                 try:
#                     # Use the existing retrieval wrapper to get chunks
#                     retrieved_chunks = self.retrieval_wrapper.retrieve_chunks_with_caching(query, top_k=5)
#                     if not retrieved_chunks:
#                         return "No relevant information found in the knowledge base."

#                     # Format the retrieved chunks
#                     formatted_chunks = []
#                     for i, chunk in enumerate(retrieved_chunks):
#                         chunk_text = f"Source {i+1}:\n"
#                         chunk_text += f"URL: {chunk.source_url}\n"
#                         chunk_text += f"Title: {chunk.page_title}\n"
#                         chunk_text += f"Section: {chunk.section_heading}\n"
#                         chunk_text += f"Content: {chunk.content}\n"
#                         chunk_text += f"Relevance Score: {chunk.score}\n"
#                         chunk_text += "---\n"
#                         formatted_chunks.append(chunk_text)

#                     return "\n".join(formatted_chunks)
#                 except Exception as e:
#                     logger.error(f"Error in retrieve_information tool: {e}")
#                     return f"Error retrieving information: {str(e)}"

#             # Create the external client and model based on configuration
#             if self.config.use_gemini:
#                 external_client = self.AsyncOpenAI(
#                     # api_key=self.config.gemini_api_key,
#                     api_key=self.config.gemini_api_key,
#                     base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
#                 )
#                 external_model = self.OpenAIChatCompletionsModel(
#                     openai_client=external_client,
#                     model=self.config.model_name,
#                 )

#                 rag_agent = self.Agent(
#                     name="RAG Book Assistant",
#                     instructions=self.system_instructions,
#                     model=external_model,
#                     tools=[retrieve_information]
#                 )
#             else:
#                 # Use default OpenAI model
#                 external_client = None
#                 external_model = None
#                 rag_agent = self.Agent(
#                     name="RAG Book Assistant",
#                     instructions=self.system_instructions,
#                     tools=[retrieve_information]
#                 )

#             # Run the agent with the input
#             # Since we're in a sync context, we need to use asyncio.run
#             async def run_agent():
#                 if self.config.use_gemini:
#                     # Use RunConfig for Gemini model
#                     config = self.RunConfig(
#                         model=external_model,
#                         model_provider=external_client
#                     )
#                     result = await self.Runner.run(rag_agent, input_text, run_config=config)
#                 else:
#                     result = await self.Runner.run(rag_agent, input_text)
#                 return result.final_output

#             # Execute the async function
#             import asyncio
#             loop = asyncio.new_event_loop()
#             asyncio.set_event_loop(loop)
#             try:
#                 content = loop.run_until_complete(run_agent())
#             finally:
#                 loop.close()

#             logger.info("Response generated successfully using OpenAI Agents SDK")
#             return content

#         except Exception as e:
#             logger.error(f"Error generating response with OpenAI Agents approach: {e}")
#             # Fall back to chat completions
#             return self._generate_response_with_chat_completions(question, retrieved_chunks)

#     def _generate_response_with_chat_completions(self, question: str, retrieved_chunks: List[RetrievedChunk]) -> str:
#         """
#         Generate a response based on the question and retrieved chunks using OpenAI Chat Completions API.

#         Args:
#             question: The original question asked
#             retrieved_chunks: List of relevant chunks retrieved from the knowledge base

#         Returns:
#             Generated response string
#         """
#         # Format the context from retrieved chunks
#         context = self._format_context_for_llm(retrieved_chunks)

#         # Prepare the message for the OpenAI API
#         messages = [
#             {"role": "system", "content": self.system_instructions},
#             {"role": "user", "content": f"Question: {question}\n\nContext: {context}\n\nPlease provide an answer based only on the provided context, citing the sources used."}
#         ]

#         try:
#             # Check if we're using an async client (when Gemini is configured)
#             if self.is_async_client:
#                 # Use async call for async client (e.g., Gemini)
#                 import asyncio

#                 async def call_async_api():
#                     response = await self.openai_client.chat.completions.create(
#                         model=self.config.model_name,  # Use configured model
#                         messages=messages,
#                         temperature=self.config.temperature,
#                         max_tokens=self.config.max_tokens
#                     )
#                     return response

#                 # Run the async function
#                 loop = asyncio.new_event_loop()
#                 asyncio.set_event_loop(loop)
#                 try:
#                     response = loop.run_until_complete(call_async_api())
#                 finally:
#                     loop.close()
#             else:
#                 # Use sync call for regular OpenAI client
#                 response = self.openai_client.chat.completions.create(
#                     model=self.config.model_name,  # Use configured model
#                     messages=messages,
#                     temperature=self.config.temperature,
#                     max_tokens=self.config.max_tokens
#                 )

#             # Extract the content from the response
#             content = response.choices[0].message.content
#             logger.info("Response generated successfully using OpenAI Chat Completions API")
#             return content

#         except Exception as e:
#             logger.error(f"Error generating response with OpenAI API: {e}")
#             # Fallback response if API call fails
#             return f"I encountered an error while generating a response. Please try again later. Error: {str(e)}"

#     def _format_context_for_llm(self, retrieved_chunks: List[RetrievedChunk]) -> str:
#         """
#         Format the retrieved chunks for input to the LLM.

#         Args:
#             retrieved_chunks: List of retrieved chunks to format

#         Returns:
#             Formatted context string
#         """
#         formatted_chunks = []
#         for i, chunk in enumerate(retrieved_chunks):
#             chunk_text = f"Source {i+1}:\n"
#             chunk_text += f"URL: {chunk.source_url}\n"
#             chunk_text += f"Title: {chunk.page_title}\n"
#             chunk_text += f"Section: {chunk.section_heading}\n"
#             chunk_text += f"Content: {chunk.content}\n"
#             chunk_text += f"Relevance Score: {chunk.score}\n"
#             chunk_text += "---\n"
#             formatted_chunks.append(chunk_text)

#         return "\n".join(formatted_chunks)

#     def _validate_content_relevance(self, question: str, retrieved_chunks: List[RetrievedChunk]) -> bool:
#         """
#         Validate that the retrieved content is relevant to the question.

#         Args:
#             question: The original question asked
#             retrieved_chunks: List of retrieved chunks to validate

#         Returns:
#             True if content is relevant, False otherwise
#         """
#         if not retrieved_chunks:
#             return False

#         # Simple keyword matching as a basic relevance check
#         question_lower = question.lower()
#         question_keywords = set(question_lower.split()[:5])  # Use first 5 words as keywords

#         # Check if any of the keywords appear in the retrieved content
#         for chunk in retrieved_chunks:
#             content_lower = chunk.content.lower()
#             chunk_keywords_found = sum(1 for keyword in question_keywords if keyword in content_lower)

#             # If at least one keyword is found in any chunk, consider it relevant
#             if chunk_keywords_found > 0:
#                 return True

#         # If no keywords were found in any chunk, content may not be relevant
#         return False

#     def _verify_response_grounding(self, response: str, retrieved_chunks: List[RetrievedChunk]) -> bool:
#         """
#         Verify that the response is grounded in the retrieved content.

#         Args:
#             response: The generated response to verify
#             retrieved_chunks: List of chunks used to generate the response

#         Returns:
#             True if response is grounded, False otherwise
#         """
#         # For now, we'll just check if the response is not empty and has sources
#         # A more sophisticated check would involve semantic similarity
#         if not response or not retrieved_chunks:
#             return False

#         # Check if response contains content that seems to be based on the retrieved chunks
#         response_lower = response.lower()

#         # Count how much of the response content overlaps with retrieved content
#         total_overlap = 0
#         for chunk in retrieved_chunks:
#             chunk_lower = chunk.content.lower()
#             # Simple overlap check - if more than 10% of the response content appears in retrieved content
#             for word in response_lower.split():
#                 if word in chunk_lower:
#                     total_overlap += 1

#         # If there's significant overlap, consider it grounded
#         response_word_count = len(response_lower.split())
#         if response_word_count > 0:
#             overlap_ratio = total_overlap / response_word_count
#             return overlap_ratio > 0.1  # At least 10% of words should have overlap

#         return True  # If response is empty, we consider it valid in this context

#     def query(self, question: str, top_k: int = None, min_score: float = None, client_id: str = "default") -> AgentResponse:
#         """
#         Main query method to ask questions to the RAG agent.

#         Args:
#             question: The book-related question to ask
#             top_k: Number of chunks to retrieve (default: from config)
#             min_score: Minimum relevance score (default: from config)
#             client_id: Identifier for the client making the request (for rate limiting)

#         Returns:
#             AgentResponse containing the answer with source attribution
#         """
#         logger.info(f"Processing query: {question[:50]}...")

#         # Check rate limit
#         if not self._check_rate_limit(client_id):
#             error_msg = "Rate limit exceeded. Please try again later."
#             logger.warning(error_msg)
#             return AgentResponse(
#                 content=error_msg,
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=[]
#             )

#         # Use defaults from config if not provided
#         top_k = top_k or self.config.top_k
#         min_score = min_score or self.config.min_score

#         # Validate the request
#         is_valid, error_msg = self._validate_request(question)
#         if not is_valid:
#             logger.error(f"Invalid request: {error_msg}")
#             return AgentResponse(
#                 content=error_msg,
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=[]
#             )

#         # Validate and preprocess the query
#         if not self.validate_query(question):
#             error_msg = "Invalid query provided"
#             logger.error(error_msg)
#             return AgentResponse(
#                 content=error_msg,
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=[]
#             )

#         processed_question = self.preprocess_query(question)

#         # Retrieve relevant chunks from knowledge base using the wrapper with caching and error handling
#         retrieved_chunks = self.retrieval_wrapper.retrieve_chunks_with_caching(processed_question, top_k)

#         if not retrieved_chunks:
#             logger.info("No relevant content found in knowledge base")
#             return AgentResponse(
#                 content="I cannot answer this question as no relevant context was found in the knowledge base.",
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=[]
#             )

#         # Validate content relevance
#         if not self._validate_content_relevance(processed_question, retrieved_chunks):
#             logger.info("Retrieved content is not relevant to the question")
#             return AgentResponse(
#                 content="I cannot answer this question as the retrieved content is not relevant to your query.",
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=[]
#             )

#         # Process retrieved chunks and generate response
#         response_content = self._generate_response(processed_question, retrieved_chunks)

#         # Verify that the response is grounded in the retrieved content
#         if not self._verify_response_grounding(response_content, retrieved_chunks):
#             logger.warning("Generated response is not properly grounded in retrieved content")
#             return AgentResponse(
#                 content="I cannot provide a properly grounded answer based on the retrieved content.",
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=retrieved_chunks
#             )

#         # Extract source URLs from retrieved chunks
#         sources = list(set([chunk.source_url for chunk in retrieved_chunks if chunk.source_url]))

#         # Ensure strict source attribution requirements are met
#         if not sources:
#             logger.warning("No sources found in retrieved content")
#             return AgentResponse(
#                 content="I found relevant content but cannot provide proper source attribution.",
#                 sources=[],
#                 grounded=False,
#                 retrieved_chunks=retrieved_chunks
#             )

#         logger.info(f"Generated response with {len(sources)} unique sources")

#         return AgentResponse(
#             content=response_content,
#             sources=sources,
#             grounded=True,
#             retrieved_chunks=retrieved_chunks
#         )

#     def health_check(self) -> Dict[str, Any]:
#         """
#         Perform a health check of the RAG agent system.

#         Returns:
#             Dictionary containing health status information
#         """
#         try:
#             # Check OpenAI client
#             openai_status = "healthy" if self.openai_client is not None else "unhealthy"

#             # Check Cohere client
#             cohere_status = "healthy" if self.cohere_client is not None else "unhealthy"

#             # Check retrieval system
#             retrieval_status = "healthy" if self.retrieval_wrapper.validate_retrieval() else "unhealthy"

#             # Overall status
#             overall_status = "healthy" if all([
#                 openai_status == "healthy",
#                 cohere_status == "healthy",
#                 retrieval_status == "healthy"
#             ]) else "unhealthy"

#             return {
#                 "status": overall_status,
#                 "timestamp": time.time(),
#                 "components": {
#                     "openai_agent": openai_status,
#                     "cohere_client": cohere_status,
#                     "qdrant_connection": retrieval_status,
#                     "retrieval_wrapper": retrieval_status
#                 }
#             }
#         except Exception as e:
#             logger.error(f"Health check failed: {e}")
#             return {
#                 "status": "unhealthy",
#                 "timestamp": time.time(),
#                 "error": str(e),
#                 "components": {
#                     "openai_agent": "unknown",
#                     "cohere_client": "unknown",
#                     "qdrant_connection": "unknown",
#                     "retrieval_wrapper": "unknown"
#                 }
#             }

#     def validate_retrieval_system(self) -> Dict[str, Any]:
#         """
#         Validate that the retrieval system is functioning correctly.

#         Returns:
#             Dictionary containing validation results
#         """
#         try:
#             return {
#                 "connection_valid": self.retrieval_wrapper.validate_retrieval(),
#                 "retrieval_success": True,  # If connection is valid, retrieval should work
#                 "metadata_valid": True,  # Assuming metadata is valid if connection works
#                 "content_relevant": True,  # This would need more sophisticated testing
#                 "overall_success": self.retrieval_wrapper.validate_retrieval()
#             }
#         except Exception as e:
#             logger.error(f"Retrieval system validation failed: {e}")
#             return {
#                 "connection_valid": False,
#                 "retrieval_success": False,
#                 "metadata_valid": False,
#                 "content_relevant": False,
#                 "overall_success": False,
#                 "error": str(e)
#             }


#     def verify_api_key(self, api_key: str) -> bool:
#         """
#         Verify the provided API key for authentication.

#         Args:
#             api_key: The API key to verify

#         Returns:
#             True if valid, False otherwise
#         """
#         # In a real implementation, you would check this against a database or other storage
#         # For now, we'll just check if it matches the OpenAI API key (not recommended for production)
#         expected_key = self.config.openai_api_key
#         return api_key == expected_key


# if __name__ == "__main__":
#     # Example usage
#     try:
#         agent = RAGAgent()
#         print("RAG Agent initialized successfully!")
#         print(f"Collection name: {agent.config.collection_name}")

#         # Test health check
#         health = agent.health_check()
#         print(f"Health check result: {health['status']}")

#         # Test query validation
#         test_query = "What is ROS 2?"
#         is_valid = agent.validate_query(test_query)
#         print(f"Query '{test_query}' is valid: {is_valid}")

#         # Test query preprocessing
#         preprocessed = agent.preprocess_query(test_query)
#         print(f"Preprocessed query: '{preprocessed}'")

#         # Test retrieval system validation
#         retrieval_validation = agent.validate_retrieval_system()
#         print(f"Retrieval system validation: {retrieval_validation['overall_success']}")

#     except ValueError as e:
#         print(f"Error initializing RAG Agent: {e}")
#     except Exception as e:
#         print(f"Error during testing: {e}")


#!/usr/bin/env python3
"""
RAG Agent using OpenAI Agents SDK + OpenRouter

Model:
- mistralai/mistral-7b-instruct:free

Architecture:
- Manual RAG
- No tools
- FastAPI-safe
"""

import os
import time
import logging
import asyncio
from typing import List, Dict, Any
from dataclasses import dataclass
from collections import defaultdict
from dotenv import load_dotenv

# =========================
# ENV + LOGGING
# =========================

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# =========================
# DATA MODELS
# =========================


@dataclass
class RetrievedChunk:
    chunk_id: str
    content: str
    source_url: str
    page_title: str
    section_heading: str
    chunk_index: int
    score: float
    original_chunk_id: str


@dataclass
class AgentResponse:
    content: str
    sources: List[str]
    grounded: bool
    retrieved_chunks: List[RetrievedChunk]


# =========================
# CONFIG
# =========================


class AgentConfig:
    def __init__(self):
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        self.qdrant_url = os.getenv("QDRANT_URL")
        self.qdrant_api_key = os.getenv("QDRANT_API_KEY")

        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME", "Rag_Chatbot_book")

        # OpenRouter model
        self.model_name = "mistralai/mistral-7b-instruct:free"

        self.temperature = 0.3
        self.max_tokens = 2048
        self.top_k = 5

        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY is required")

        if not self.qdrant_url:
            raise ValueError("QDRANT_URL is required")


# =========================
# RETRIEVAL WRAPPER
# =========================


class RetrievalWrapper:
    def __init__(self, retrieval_validator):
        self.retrieval_validator = retrieval_validator
        self.cache: Dict[str, List[RetrievedChunk]] = {}

    def retrieve(self, query: str, top_k: int) -> List[RetrievedChunk]:
        cache_key = f"{query}:{top_k}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        chunks = self.retrieval_validator.retrieve_chunks(query, top_k)
        self.cache[cache_key] = chunks
        return chunks

    def validate(self) -> bool:
        return self.retrieval_validator.validate_connection()


# =========================
# RAG AGENT
# =========================


class RAGAgent:
    def __init__(self, config: AgentConfig | None = None):
        logger.info("Initializing OpenRouter RAG Agent")

        self.config = config or AgentConfig()

        # ---- OpenAI Agents SDK ----
        from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel

        self.Agent = Agent
        self.Runner = Runner

        # OpenRouter OpenAI-compatible client
        self.client = AsyncOpenAI(
            api_key=self.config.openrouter_api_key,
            base_url="https://openrouter.ai/api/v1",
        )

        self.model = OpenAIChatCompletionsModel(
            openai_client=self.client,
            model=self.config.model_name,
        )

        self.system_instructions = """
You are a book RAG assistant.

Rules:
- Use ONLY the provided context
- If the answer is not present, say you cannot answer
- Cite sources explicitly
- Do not use external knowledge
"""

        # Rate limiting (soft)
        self.rate_window = 60
        self.max_requests = 60
        self.requests = defaultdict(list)

        # Retrieval
        self._init_retrieval()

        logger.info("OpenRouter RAG Agent ready")

    # =========================
    # RETRIEVAL INIT
    # =========================

    def _init_retrieval(self):
        from retrieve import RetrievalValidator

        self.retrieval = RetrievalWrapper(RetrievalValidator())

    # =========================
    # RATE LIMIT
    # =========================

    def _rate_limit_ok(self, client_id: str) -> bool:
        now = time.time()
        self.requests[client_id] = [
            t for t in self.requests[client_id] if now - t < self.rate_window
        ]
        if len(self.requests[client_id]) >= self.max_requests:
            return False
        self.requests[client_id].append(now)
        return True

    # =========================
    # CONTEXT FORMAT
    # =========================

    def _format_context(self, chunks: List[RetrievedChunk]) -> str:
        parts = []
        for i, c in enumerate(chunks, 1):
            parts.append(
                f"""Source {i}:
URL: {c.source_url}
Title: {c.page_title}
Section: {c.section_heading}
Content: {c.content}
---"""
            )
        return "\n".join(parts)

    # =========================
    # AGENT EXECUTION
    # =========================

    async def _run_agent(self, prompt: str) -> str:
        agent = self.Agent(
            name="OpenRouter RAG Agent",
            instructions=self.system_instructions,
            model=self.model,
            tools=[],  # REQUIRED
        )

        result = await self.Runner.run(agent, prompt)
        return result.final_output

    def _run_async_safe(self, coro):
        try:
            loop = asyncio.get_running_loop()
            return loop.create_task(coro)
        except RuntimeError:
            return asyncio.run(coro)

    # =========================
    # PUBLIC QUERY
    # =========================

    def query(self, question: str, client_id: str = "default") -> AgentResponse:
        if not self._rate_limit_ok(client_id):
            return AgentResponse(
                content="Rate limit exceeded.",
                sources=[],
                grounded=False,
                retrieved_chunks=[],
            )

        question = question.strip()
        if len(question) < 3:
            return AgentResponse(
                content="Invalid question.",
                sources=[],
                grounded=False,
                retrieved_chunks=[],
            )

        chunks = self.retrieval.retrieve(question, self.config.top_k)

        if not chunks:
            return AgentResponse(
                content="I cannot answer because no relevant context was found.",
                sources=[],
                grounded=False,
                retrieved_chunks=[],
            )

        context = self._format_context(chunks)

        prompt = f"""
Answer ONLY using the context below.
If the answer is not present, say you cannot answer.

Context:
{context}

Question:
{question}
"""

        try:
            content = self._run_async_safe(self._run_agent(prompt))
        except Exception as e:
            logger.error(f"OpenRouter error: {e}")
            return AgentResponse(
                content="An internal error occurred.",
                sources=[],
                grounded=False,
                retrieved_chunks=chunks,
            )

        sources = list({c.source_url for c in chunks if c.source_url})

        return AgentResponse(
            content=str(content),
            sources=sources,
            grounded=True,
            retrieved_chunks=chunks,
        )

    # =========================
    # HEALTH
    # =========================

    def health_check(self) -> Dict[str, Any]:
        return {
            "status": "healthy" if self.retrieval.validate() else "unhealthy",
            "model": self.config.model_name,
            "timestamp": time.time(),
        }


# =========================
# LOCAL TEST
# =========================

if __name__ == "__main__":
    agent = RAGAgent()
    print("OpenRouter agent initialized")

    response = agent.query("What is ROS 2?")
    print(response.content)
